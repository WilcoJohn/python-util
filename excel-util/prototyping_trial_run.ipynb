{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dfcd8c2-98e6-4004-8f1f-9d2cf508ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\WilcoSievers\\OneDrive - MOYO Business Advisory\\Documents\\GitHub\\python-util\\excel-util\\\\\" )\n",
    "import exutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c3746b7d-d8d7-4785-8cf3-c1dc36f2c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import openpyxl\n",
    "import numpy\n",
    "import fnmatch\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37e6c54-ac4d-4f16-a28a-f2542a097fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_columns(file_paths : list[str], target_sheets : list[str], start_points : list , end_points : list[str] = ['NaN'], **kwargs):\n",
    "    \"\"\"\n",
    "    Takes in \n",
    "    \n",
    "    *Args:\n",
    "        \n",
    "        file_paths = List of file paths that need to be searched\n",
    "        \n",
    "        target_sheets = Sheet name to target, if no hit is recieved for first val continue to next val and search unit hit.\n",
    "        \n",
    "        start_points = List[str] of multiple values to search. These points are used as an anchor points for when selecting values. If the first value in the iterable returns None, move onto the next value.\n",
    "        \n",
    "        end_points = List[str] of multiple values to search. These points are used as an anchor points for when selecting values. If the first value in the iterable returns None, move onto the next value.\n",
    "\n",
    "    **kwargs:\n",
    "        \n",
    "        start_pivot = Tuple of (int, int) as (row, col) - used to pivot from start point \n",
    "        \n",
    "        end_pivot = Tuple of (int, int) as (row, col) - used to pivot from end point \n",
    "        \n",
    "        searched_columns = Known columns of previous search results  \n",
    "        \n",
    "        searched_files = list[str] - list of files that have been searched and values that have\n",
    "        \n",
    "        start_default = (row, col) - Start value to default to if no resutls are found. If  'start_default' is not given, and the start points give no result, an error will be raised.\n",
    "        \n",
    "        end_default = (row, col) - Start value to default to if no resutls are found. If  'start_default' is not given, and the start points give no result, an error will be raised\n",
    "        \n",
    "        columns_to_exclue = Columns to remove, as there are certain cell values that are just thrown around in the bloody sheet.\n",
    "    \"\"\"\n",
    "    # import dependant functions\n",
    "    from openpyxl import load_workbook\n",
    "    from exutil import is_similar, search_excl_val, exc_coord_to_rc, get_excl_sheet_vals, filter_files\n",
    "    from datetime import datetime\n",
    "\n",
    "    \n",
    "    # Prelude variables used for various downstream processes\n",
    "    start_point, end_point = {'row' : 0, 'col' : 0}, {'row' : 0, 'col' : 0};\n",
    "    start_point_pivot, end_point_pivot = {'row' : 0, 'col' : 0}, {'row' : 0, 'col' : 0};\n",
    "    start_point_default, end_point_default = {'row' : 0, 'col' : 0}, {'row' : 0, 'col' : 0};\n",
    "    start_default, end_default = {'row' : None, 'col' : None}, {'row' : None, 'col' : None};\n",
    "    \n",
    "    column_result = [];\n",
    "    column_source = [];\n",
    "    searched_files = [];\n",
    "    columns_to_exclue = [];\n",
    "\n",
    "    \n",
    "    for key, val in kwargs.items():\n",
    "        match key:\n",
    "            case 'start_pivot':\n",
    "                start_point_pivot['row'], start_point_pivot['col'] = val;\n",
    "            case 'end_pivot':\n",
    "                end_point_pivot['row'], end_point_pivot['col'] = val;\n",
    "            case 'searched_files':\n",
    "                searched_files = searched_files + val; # append the files\n",
    "            case 'start_default':\n",
    "                start_point_default['row'], start_point_default['col'] = val;\n",
    "            case 'end_default':\n",
    "                end_point_default['row'], end_point_default['col'] = val;\n",
    "            case 'searced_columns':\n",
    "                column_result = searched_columns;\n",
    "            case 'columns_to_exclue':\n",
    "                columns_to_exclue = val;\n",
    "            case _:\n",
    "                raise NameError(f\"Unkown values '{key}' - '{val}' fed. Expected **kwargs = [start_pivot, end_pivot, searched_files, start_default, end_default]\")\n",
    "    \n",
    "    # remove file paths already searched\n",
    "    files_paths_to_search = file_paths;\n",
    "\n",
    "\n",
    "    \n",
    "    # this is inneficient method. Search throygh file paths list and filter out values that have been searched\n",
    "    for file_i in searched_files:\n",
    "        file_slice_idx = file_i.rfind('\\\\') if file_i.rfind('\\\\')  != -1 else 0;\n",
    "        files_paths_to_search = filter_files(files_paths_to_search,                    # the path list to iterate and make shrink\n",
    "                                             '*' + file_i[file_slice_idx::] + '*',     # the pattern to search for, just look at final file\n",
    "                                             matchPattern = False)\n",
    "\n",
    "    \n",
    "    for file_i in files_paths_to_search:\n",
    "        try: \n",
    "            active_workbook = load_workbook(file_i);\n",
    "        except PermissionError as pe:\n",
    "            print(pe); # a file is open in another app, thus permission is denied, continue on\n",
    "            continue;\n",
    "\n",
    "        # Search for most relevenat sheet\n",
    "        active_sheet_name = None;\n",
    "        for sheet_ii in target_sheets:\n",
    "            similar_sheets, simalr_sheet_scores = is_similar(sheet_ii, active_workbook.sheetnames, return_similarrity_score=True);\n",
    "            if similar_sheets is not None:\n",
    "                active_sheet_name = similar_sheets[simalr_sheet_scores.argmax()];\n",
    "                break;\n",
    "        if active_sheet_name is None:\n",
    "            print(f\"Could not find sheet targets : {target_sheets} for {file_i}\");\n",
    "            continue;\n",
    "        \n",
    "\n",
    "        # set active sheet\n",
    "        active_sheet = active_workbook[active_sheet_name];\n",
    "\n",
    "        \n",
    "        # search for start anchor pivot point\n",
    "        for start_pii in start_points:\n",
    "            excl_coord, _ = search_excl_val(active_sheet, [start_pii], return_first_hit = True);\n",
    "            if excl_coord is not None:\n",
    "                start_point['row'], start_point['col'] =  exc_coord_to_rc(excl_coord);\n",
    "                start_point['row'] = start_point['row'] + start_point_pivot['row']; \n",
    "                start_point['col'] = start_point['col'] + start_point_pivot['col']; \n",
    "                break;\n",
    "        # Ensure start point does not have any errors\n",
    "        if (start_point['row'] == 0) and (start_point['col'] == 0):\n",
    "            if (start_point_default['row'] is None) and (start_point_default['col'] is None):\n",
    "                raise UserWarning(f\"Could not find start values '{start_points}' in '{active_sheet_name}' in '{file_i}'\")\n",
    "            else:\n",
    "                start_point = start_point_default;\n",
    "\n",
    "                \n",
    "        # search for end anchor pivot point\n",
    "        for end_pii in end_points:\n",
    "            excl_coord, _ = search_excl_val(active_sheet, [end_pii], return_first_hit = True);\n",
    "\n",
    "            if excl_coord is not None:\n",
    "                end_point['row'], end_point['col'] =  exc_coord_to_rc(excl_coord);\n",
    "                end_point['row'] += end_point_pivot['row']; \n",
    "                end_point['col'] += end_point_pivot['col']; \n",
    "                break;\n",
    "        # Ensure end point does not have any errors\n",
    "        if (end_point['row'] == 0) and (end_point['col'] == 0):\n",
    "            if (end_point_default['row'] is None) and (end_point_default['col'] is None):\n",
    "                raise UserWarning(f\"Could not find end values '{end_points}' in '{active_sheet_name}' in '{file_i}'\")\n",
    "            else:\n",
    "                end_point =  end_point_default;\n",
    "\n",
    "        # extract the values, columns are in signle line, thus use start col as singular column\n",
    "        extracted_columns = get_excl_sheet_vals(active_sheet,   # columns are in singular column\n",
    "                                               (start_point['row'], start_point['col']),\n",
    "                                               (end_point['row'], start_point['col']));\n",
    "\n",
    "        # remove any None/ empty values\n",
    "        extracted_columns = [colVal for colVal in extracted_columns if colVal]; \n",
    "        # rek0ve date values\n",
    "        extracted_columns = [colVal for colVal in extracted_columns if not isinstance(colVal, datetime)]; \n",
    "        # remove any weird values\n",
    "        extracted_columns = [colVal for colVal in extracted_columns if (str(colVal).find('=') == -1) and str(colVal).find(',') == -1];\n",
    "\n",
    "        \n",
    "        for new_col_ii in extracted_columns:\n",
    "            if new_col_ii not in column_result:\n",
    "                column_result.append(new_col_ii);\n",
    "                column_source.append(file_i);\n",
    "\n",
    "    # final cleanup for values\n",
    "    col_source_list = [];\n",
    "    col_list = [];\n",
    "    for col_val_ii, path_source_ii in zip(column_result, column_source):\n",
    "        if col_val_ii not in columns_to_exclue:\n",
    "            col_source_list.append(path_source_ii[path_source_ii.rfind('\\\\')::]);   # just take the final column/ slice\n",
    "            col_list.append(col_val_ii);\n",
    "            \n",
    "    return col_list, col_source_list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d36650c-3460-45c1-a4be-dd2e43b53f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_data(data : pandas.DataFrame, files_to_search : list[str], start_col, end_col, columns_to_extract):\n",
    "    return;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead0d89c-23f1-45b4-a598-d0dc3def9149",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\WilcoSievers\\OneDrive - MOYO Business Advisory\\Documents\\Assigments\\2025-07-04 - Mine data analysis\\Data\\Combine data\\KPI FILE 19 Oct 24\"\n",
    "file_pattern = \"*.xlsx\"\n",
    "all_file_paths = exutil.filter_files(base_path, file_pattern)\n",
    "\n",
    "\n",
    "# Values for columns searcg\n",
    "sheet_targets = [\"densities & reagents\", \"densities and reagents\"];\n",
    "start_points = [\"SP\"];\n",
    "end_points = [\"AG HT %CaF2\", \"HT AVG\"]; \n",
    "\n",
    "known_column_list = [1163, 1164, 'CRT', 'ARF', 'CL1 Feed', 'CL2 Feed', 'CL2 concentrate', 'CL3 concentrate', 'CL4 concentrate', 'CL5 concentrate', 'Refloat feed', 'Refloat 1 concentrate', 'Refloat 2 concentrate', 'Refloat 3 concentrate', 'AR Collector ', 'CL1 Collector ', 'Collector (CCT) ', 'CL1 Dextrin / CL2', 'CL2 Dextrin ', 'CL3 Dextrin ', 'CL4 Dextrin ', 'Coagulant ', 'RF2 Dextrin', 'CL5 Dispersant', 'Surge Tank Density', 'WHIMS Feed Density', 'Tails Flocculant Flow(L/min)', 'Flocculant Strength', 'Soda ash flow', 'Metspar concentrate', 'Surge Tank Density(SCADA)', 'Conditioner 1(Manual)', 'Conditioner 1', 'FLOC WTP1', 'FLOC WTP2', 'Coag WTP1', 'Coag WTP2', 'Flocculant', 'Flocculant Dosing Strength', ' ', 'CL6 concentrate(SCADA)', 'AR Dextrin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "732eaf5f-3349-4d0a-9545-da07831f1df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = pandas.read_csv(\"Avg_density_and_reactants_2023_2024_2025_col.csv\");\n",
    "\n",
    "previously_searched_files = pandas.read_csv(\"searched_files.csv\").File;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7720a04c-0f0b-498e-b81b-df6b3abe877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsearched_file_paths = [];\n",
    "\n",
    "for file_searched in all_file_paths:\n",
    "    if fnmatch.filter(searched_files, '*'+file_searched[file_searched.rfind('\\\\'):]):      # check if serched file is in all the list\n",
    "        continue;                                              # if not, append it \n",
    "    else:\n",
    "        unsearched_file_paths.append(file_searched);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e68cd0d8-f613-45f9-9700-c5e00a41e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, b = search_for_columns(all_file_paths, sheet_targets, \n",
    "                           start_points, end_points,\n",
    "                           start_pivot=(0,-3), end_pivot=(-1,0), \n",
    "                           searched_files =  previously_searched_files.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0b8b0de-d73a-4f3a-9a5f-6cc26bfba08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = add_new_data(data_original, files_to_add, path_to_save, );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fa8890f-a552-4e73-a1e0-1ff874d5ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_original;\n",
    "files_to_search = unsearched_file_paths;\n",
    "\n",
    "columns_to_search = known_column_list;\n",
    "start_col_searchval = datetime.time(7);\n",
    "end_col_searchval  = datetime.time(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f56dc20-9589-4106-bd37-86dd5301d2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 13] Permission denied: 'C:\\\\Users\\\\WilcoSievers\\\\OneDrive - MOYO Business Advisory\\\\Documents\\\\Assigments\\\\2025-07-04 - Mine data analysis\\\\Data\\\\Combine data\\\\KPI FILE 19 Oct 24\\\\2023\\\\Densities Reagents 08 March 23.xlsx'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'file_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m     buffer_dict \u001b[38;5;241m=\u001b[39m buffer_dict_empty\u001b[38;5;241m.\u001b[39mcopy(); \n\u001b[0;32m    110\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime();\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy\u001b[38;5;241m.\u001b[39msize(file_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy\u001b[38;5;241m.\u001b[39mround((t1\u001b[38;5;241m-\u001b[39mt0)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file_paths' is not defined"
     ]
    }
   ],
   "source": [
    "t0 = time.time();\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from exutil import is_similar, search_excl_val, exc_coord_to_rc, get_excl_sheet_vals, filter_files\n",
    "from numpy import mean\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "buffer_list = [];\n",
    "\n",
    "buffer_dict_empty = {str(col) : None for col in columns_to_search};\n",
    "buffer_dict_empty['Date'] = None;\n",
    "buffer_dict_empty['File'] = None;\n",
    "\n",
    "buffer_dict = buffer_dict_empty.copy();\n",
    "\n",
    "for file_i in files_to_search:\n",
    "    try: \n",
    "        active_workbook = load_workbook(file_i);\n",
    "    except PermissionError as pe:\n",
    "        print(pe);                         # file cannot be accessed, thus permission is denied, continue on to next file\n",
    "        continue;\n",
    "        \n",
    "    # Search for most relevenat sheet\n",
    "    active_sheet_name = None;\n",
    "    for sheet_ii in sheet_targets:\n",
    "        similar_sheets, simalr_sheet_scores = is_similar(sheet_ii, active_workbook.sheetnames, return_similarrity_score=True);\n",
    "        if similar_sheets is not None:\n",
    "            active_sheet_name = similar_sheets[simalr_sheet_scores.argmax()];\n",
    "            break;\n",
    "    if active_sheet_name is None:\n",
    "        print(f\"Could not find sheet targets : {target_sheets} for {file_i}\");\n",
    "        continue;\n",
    "    # set active sheet\n",
    "    active_sheet = active_workbook[active_sheet_name];\n",
    "\n",
    "\n",
    "    \n",
    "    # search for the column coordinates values where data is stored\n",
    "    try:\n",
    "        # search for time as time val\n",
    "        start_coordinate, _ = search_excl_val(active_sheet, [start_col_searchval]); \n",
    "    except TypeError:\n",
    "        try:\n",
    "            # search for time as datetime val\n",
    "             start_coordinate, _  = search_excl_val(active_sheet, [datetime.datetime.combine(datetime.date(1900,1,1), start_col_searchval)]);\n",
    "        except TypeError:\n",
    "            # search for time as string val\n",
    "            start_coordinate, _ = search_excl_val(active_sheet, [start_col_searchval.strftime(\"%Hh%M\")]);\n",
    "    try:\n",
    "        # search for time as time val\n",
    "        end_coordinate, _   = search_excl_val(active_sheet, [end_col_searchval]);   \n",
    "    except TypeError:\n",
    "        try:\n",
    "            # search for time as datetime val\n",
    "            end_coordinate, _  = search_excl_val(active_sheet, [datetime.datetime.combine(datetime.date(1900,1,1), end_col_searchval)]);\n",
    "        except TypeError:\n",
    "            # search for time as string val\n",
    "            end_coordinate, _  = search_excl_val(active_sheet, [end_col_searchval.strftime(\"%Hh%M\")]);\n",
    "\n",
    "    \n",
    "    if isinstance(start_coordinate, str):\n",
    "        _, start_col = exc_coord_to_rc(start_coordinate);\n",
    "    if isinstance(end_coordinate, str):\n",
    "        _, end_col = exc_coord_to_rc(end_coordinate);\n",
    "    # The time format changes across sheets for some reasons, thus also search for time with form \"14h00\" when 14:00 doesn't\n",
    "    # give a hit. This is a quick workaround to get the correct column values \n",
    "    if (end_col-start_col)==1 or (): # check if column sare the same, which is a n indicaion of \n",
    "        start_coordinate, _ = search_excl_val(active_sheet, [start_col_searchval.strftime(\"%Hh%M\")])\n",
    "        end_coordinate, _   = search_excl_val(active_sheet, [end_col_searchval.strftime(\"%Hh%M\")]);       \n",
    "        if isinstance(start_coordinate, str):\n",
    "            _, start_col = exc_coord_to_rc(start_coordinate);       \n",
    "        if isinstance(end_coordinate, str):\n",
    "            _, end_col = exc_coord_to_rc(end_coordinate);\n",
    "    # more error handling will be needed, however, for now, this i sok\n",
    "\n",
    "\n",
    "    # Time to extract the data\n",
    "    for var_to_search in columns_to_search:\n",
    "        coordinate, _ = search_excl_val(active_sheet, [var_to_search])\n",
    "        numeric_vals = [];\n",
    "        values_extracted = [];\n",
    "        if coordinate:\n",
    "            row_select, _ = exc_coord_to_rc(coordinate);\n",
    "            values_extracted = get_excl_sheet_vals(active_sheet, (row_select, start_col), (row_select, end_col));\n",
    "            # extract numeric vals\n",
    "\n",
    "            for val in values_extracted:\n",
    "                try:\n",
    "                    numeric_vals.append(float(val));\n",
    "                except (ValueError, TypeError):\n",
    "                    continue;         \n",
    "            mean_val =  mean(numeric_vals) if (numeric_vals != []) else 0;  # give mean result, if  numeric vals is not empty   \n",
    "        else:\n",
    "            mean_val = 0;\n",
    "        # print(f\"{var_to_search} - {mean_val} - {coordinate} - {values_extracted} - {numeric_vals}\")\n",
    "        buffer_dict[str(var_to_search)] = mean_val;\n",
    "    \n",
    "    # Extract all the date values\n",
    "    date_values = search_excl_val(active_sheet, [datetime.date(1990,1,1)], return_first_hit=False, return_all_vals=True, threshold=0.1);\n",
    "    \n",
    "    buffer_dict['Date'] = max(date_values['Value']); # append the most up to date values, which is likely the max\n",
    "    buffer_dict['File'] = file_i;\n",
    "    # append the values to a list comprised of dicts\n",
    "    buffer_list.append(buffer_dict.copy());\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "137d0eac-430a-4a38-8a9e-dc76a43bc03a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'datetime.date' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m all_files_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mto_datetime(all_files_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate;\n\u001b[0;32m      5\u001b[0m data_combined \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mconcat([data, all_files_df]);\n\u001b[1;32m----> 6\u001b[0m data_combined\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m);\n\u001b[0;32m      7\u001b[0m data_combined\n\u001b[0;32m     10\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime();\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:7200\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   7197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ascending, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m   7198\u001b[0m         ascending \u001b[38;5;241m=\u001b[39m ascending[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 7200\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m nargsort(\n\u001b[0;32m   7201\u001b[0m         k, kind\u001b[38;5;241m=\u001b[39mkind, ascending\u001b[38;5;241m=\u001b[39mascending, na_position\u001b[38;5;241m=\u001b[39mna_position, key\u001b[38;5;241m=\u001b[39mkey\n\u001b[0;32m   7202\u001b[0m     )\n\u001b[0;32m   7203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   7204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\sorting.py:439\u001b[0m, in \u001b[0;36mnargsort\u001b[1;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[0;32m    437\u001b[0m     non_nans \u001b[38;5;241m=\u001b[39m non_nans[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    438\u001b[0m     non_nan_idx \u001b[38;5;241m=\u001b[39m non_nan_idx[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 439\u001b[0m indexer \u001b[38;5;241m=\u001b[39m non_nan_idx[non_nans\u001b[38;5;241m.\u001b[39margsort(kind\u001b[38;5;241m=\u001b[39mkind)]\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ascending:\n\u001b[0;32m    441\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'datetime.date' and 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "all_files_df = pandas.DataFrame(buffer_list)\n",
    "all_files_searched = all_files_df['File'];\n",
    "\n",
    "all_files_df['Date'] = pandas.to_datetime(all_files_df['Date']).dt.date;\n",
    "data_combined = pandas.concat([data, all_files_df]);\n",
    "data_combined.sort_values(by='Date', ascending=True, inplace=True);\n",
    "data_combined.drop_duplicates(by='Date', keep='First', inplace=True)\n",
    "data_panel_beat = data_combined.set_index('Date', inplace=True);\n",
    "data_panel_beat.to_csv(\"combined-data-sorted.csv\");\n",
    "\n",
    "\n",
    "df_buffer_dates = pandas.DataFrame(buffer_dict_empty);\n",
    "df_buffer_dates['Date'] = pandas.date_range(datetime.date(2023,1,1), datetime.date.today());\n",
    "df_buffer_dates['Date'] = [ date_i.date() for date_i in df_buffer_dates['Date'] ];\n",
    "df_buffer_dates.sort_values(by='Date', ascending=True, inplace=True);\n",
    "df_buffer_dates.set_index('Date', inplace=True);\n",
    "df_buffer_dates['File'] = df_buffer_dates['File'].astype(str);\n",
    "\n",
    "\n",
    "# set up logical array for data sorting \n",
    "log_arr = (df_buffer_dates.index == data_panel_beat.index[0]);\n",
    "\n",
    "# estimate what date values are populated and which are not.\n",
    "for date_i in data_panel_beat.index:\n",
    "    log_arr = log_arr + (date_i == df_buffer_dates.index);\n",
    "\n",
    "df_to_include = df_buffer_dates[~log_arr];\n",
    "df_to_include['File'] = 'Interp';\n",
    "\n",
    "\n",
    "DF_before_interp = pandas.concat([data_panel_beat, df_to_include])\n",
    "DF_before_interp.sort_index(inplace=True);\n",
    "\n",
    "DF_interp = DF_before_interp.copy();\n",
    "DF_interp.infer_objects(copy=False); # required\n",
    "DF_interp.interpolate(method='linear', limit=600, limit_direction='both', inplace=True);\n",
    "DF_interp.to_csv(\"Interpolated_values\")\n",
    "\n",
    "t1 = time.time();\n",
    "print(f\"Time taken for {numpy.size(files_to_search)} files - {numpy.round((t1-t0)/60,2)} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99abcec-a1ff-403a-b404-8a1f451964b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_searched = data_Frame['File'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2238f355-2714-406f-af2a-261292e5a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pandas.DataFrame(buffer_list);\n",
    "data_frame.to_csv(\"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
